{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from open ai cookbook:\n",
    "# https://cookbook.openai.com/examples/assistants_api_overview_python\n",
    "\n",
    "# inits\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def show_json(obj):\n",
    "    display(json.loads(obj.model_dump_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 - create an assistant with file search enabled\n",
    "assistant = client.beta.assistants.create(\n",
    "    name = \"Machine learning researcher\",\n",
    "    instructions = \"You are a machine learning researcher. Answer questions using the research paper.\",\n",
    "    tools = [{\"type\": \"file_search\"}],\n",
    "    model = \"gpt-4o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Upload files and add them to a Vector Store\n",
    "# https://platform.openai.com/docs/assistants/tools/file-search/step-2-upload-files-and-add-them-to-a-vector-store\n",
    "\n",
    "vector_store = client.beta.vector_stores.create(name=\"memgpt_research\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"memgpt-paper.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "\n",
    "# file = client.files.create(\n",
    "#     file=open(\"memgpt-paper.pdf\", \"rb\"),\n",
    "#     purpose = \"assistants\"\n",
    "# )\n",
    "# print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: Update the assistant to use the new Vector Store\n",
    "# To make the files accessible to your assistant, update the assistant’s tool_resources with the new vector_store id.\n",
    "\n",
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_6QU1KDAL8rSyfxHRrQKNimBZ'])\n"
     ]
    }
   ],
   "source": [
    "# step 4 - create a thread\n",
    "\n",
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"memgpt-paper.pdf\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Summarize the research paper\",\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(thread.tool_resources.file_search)\n",
    "\n",
    "# thread = client.beta.threads.create()\n",
    "# print(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary of Research Paper: MemGPT: Towards LLMs as Operating Systems\n",
      "\n",
      "#### Abstract\n",
      "MemGPT is proposed to overcome the limitations of large language models (LLMs), which typically have constrained context windows. Leveraging techniques from traditional operating systems (OS), such as hierarchical memory management and virtual memory paging, MemGPT manages different storage tiers to extend the effective context for LLMs. This allows for superior performance in tasks like extensive document analysis and long-term conversational interactions[0].\n",
      "\n",
      "#### Introduction\n",
      "Recent advances in LLMs, including transformer architectures, have significantly enhanced conversational AI. However, their limited fixed-length context windows restrict their effectiveness in long conversations and document analysis. Approaches to extend context length result in increasing computational costs, both in time and memory, due to the self-attention mechanism of transformers. Consequently, there is a need for alternative solutions to support extended context without incurring prohibitive costs[1][2].\n",
      "\n",
      "#### MemGPT's Architecture\n",
      "MemGPT’s design emulates an operating system with a multi-level memory architecture, split into main context (in-context for the LLM) and external context (out-of-context data stored externally). MemGPT enables LLMs to manage their own memory through function calls, akin to how an OS manages memory, thus providing a semblance of infinite context[3][4].\n",
      "\n",
      "#### Evaluation\n",
      "MemGPT's performance was evaluated on:\n",
      "1. **Document Analysis**: MemGPT successfully processed extensive documents beyond the context limit of standard LLMs by paging relevant data in and out of memory. This allowed it to outperform baseline models constrained by fixed context windows.\n",
      "2. **Conversational Agents**: MemGPT facilitated maintaining long-term memory and personality consistency throughout extended dialogues, addressing the limitations of existing LLM conversational agents[5][6].\n",
      "\n",
      "#### Results\n",
      "1. **Deep Memory Retrieval Task**: MemGPT excelled in tasks requiring the retrieval of information from prior sessions, significantly outperforming baseline models (e.g., MemGPT with GPT-4 achieved an accuracy of 92.5% compared to GPT-4's 32.1%)[7].\n",
      "2. **Nested Key-Value Retrieval Task**: Demonstrated MemGPT's capability in handling multi-hop lookups, integrating information from multiple queries effectively, showcasing its ability to manage complex retrieval tasks better than baseline models[8][9].\n",
      "\n",
      "#### Conclusion\n",
      "MemGPT demonstrates that by borrowing concepts from OS design, it is possible to overcome the inherent limitations of LLM context windows. Future research directions include extending MemGPT's application to more domains, integrating various memory tiers such as databases or caches, and refining control flow and memory management policies further. MemGPT thus represents a promising avenue for enhancing the capabilities of LLMs within their fundamental limits[10][0].\n",
      "[0] memgpt-paper.pdf\n",
      "[1] memgpt-paper.pdf\n",
      "[2] memgpt-paper.pdf\n",
      "[3] memgpt-paper.pdf\n",
      "[4] memgpt-paper.pdf\n",
      "[5] memgpt-paper.pdf\n",
      "[6] memgpt-paper.pdf\n",
      "[7] memgpt-paper.pdf\n",
      "[8] memgpt-paper.pdf\n",
      "[9] memgpt-paper.pdf\n",
      "[10] memgpt-paper.pdf\n",
      "[11] memgpt-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# step 5: Crate a run and check the output (without streaming)\n",
    "\n",
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
